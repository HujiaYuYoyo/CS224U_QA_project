{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freebase knowledge base\n",
    "\n",
    "# <subject>  <predicate>  <object>\n",
    "\n",
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "rel_ext_data_home = '../cs224u/rel_ext_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading KB triples from ../cs224u/rel_ext_data/kb.tsv.gz ...\n",
      "Read 56575 KB triples\n"
     ]
    }
   ],
   "source": [
    "KBTriple = namedtuple('KBTriple', 'rel, sbj, obj')\n",
    "\n",
    "def read_kb_triples():\n",
    "    kb_triples = []\n",
    "    path = os.path.join(rel_ext_data_home, 'kb.tsv.gz')\n",
    "    print('Reading KB triples from {} ...'.format(path))\n",
    "    with gzip.open(path) as f:\n",
    "        for line in f:\n",
    "            rel, sbj, obj = line[:-1].decode('utf-8').split('\\t')\n",
    "            kb_triples.append(KBTriple(rel, sbj, obj))\n",
    "    print('Read {} KB triples'.format(len(kb_triples)))\n",
    "    return kb_triples\n",
    "\n",
    "kb_triples = read_kb_triples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KB():\n",
    "\n",
    "    def __init__(self, kb_triples):\n",
    "        self._kb_triples = kb_triples\n",
    "        self._all_relations = []\n",
    "        self._all_entity_pairs = []\n",
    "        self._kb_triples_by_relation = {}\n",
    "        self._kb_triples_by_entities = {}\n",
    "        self._collect_all_entity_pairs()\n",
    "        self._index_kb_triples_by_relation()\n",
    "        self._index_kb_triples_by_entities()\n",
    "\n",
    "    def _collect_all_entity_pairs(self):\n",
    "        pairs = set()\n",
    "        for kbt in self._kb_triples:\n",
    "            pairs.add((kbt.sbj, kbt.obj))\n",
    "        self._all_entity_pairs = sorted(list(pairs))\n",
    "        \n",
    "    def _index_kb_triples_by_relation(self):\n",
    "        for kbt in self._kb_triples:\n",
    "            if kbt.rel not in self._kb_triples_by_relation:\n",
    "                self._kb_triples_by_relation[kbt.rel] = []\n",
    "            self._kb_triples_by_relation[kbt.rel].append(kbt)\n",
    "        self._all_relations = sorted(list(self._kb_triples_by_relation))\n",
    "    \n",
    "    def _index_kb_triples_by_entities(self):\n",
    "        for kbt in self._kb_triples:\n",
    "            if kbt.sbj not in self._kb_triples_by_entities:\n",
    "                self._kb_triples_by_entities[kbt.sbj] = {}\n",
    "            if kbt.obj not in self._kb_triples_by_entities[kbt.sbj]:\n",
    "                self._kb_triples_by_entities[kbt.sbj][kbt.obj] = []\n",
    "            self._kb_triples_by_entities[kbt.sbj][kbt.obj].append(kbt)\n",
    "\n",
    "    def get_triples(self):\n",
    "        return iter(self._kb_triples)\n",
    "        \n",
    "    def get_all_relations(self):\n",
    "        return self._all_relations\n",
    "            \n",
    "    def get_all_entity_pairs(self):\n",
    "        return self._all_entity_pairs\n",
    "            \n",
    "    def get_triples_for_relation(self, rel):\n",
    "        try:\n",
    "            return self._kb_triples_by_relation[rel]\n",
    "        except KeyError:\n",
    "            return []\n",
    "\n",
    "    def get_triples_for_entities(self, e1, e2):\n",
    "        try:\n",
    "            return self._kb_triples_by_entities[e1][e2]\n",
    "        except KeyError:\n",
    "            return []\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'KB with {} triples'.format(len(self._kb_triples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KB with 56575 triples"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kb = KB(kb_triples)\n",
    "kb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "all_relations = kb.get_all_relations()\n",
    "print(len(all_relations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        2140 adjoins\n",
      "        3316 author\n",
      "         637 capital\n",
      "       22489 contains\n",
      "        4958 film_performance\n",
      "        2404 founders\n",
      "        1012 genre\n",
      "        3280 has_sibling\n",
      "        3774 has_spouse\n",
      "        3153 is_a\n",
      "        1981 nationality\n",
      "        2013 parents\n",
      "        1388 place_of_birth\n",
      "        1031 place_of_death\n",
      "        1526 profession\n",
      "        1473 worked_at\n"
     ]
    }
   ],
   "source": [
    "for rel in all_relations:\n",
    "    print('{:12d} {}'.format(len(kb.get_triples_for_relation(rel)), rel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('adjoins', 'Siegburg', 'Bonn')\n",
      "('author', 'Uncle_Silas', 'Sheridan_Le_Fanu')\n",
      "('capital', 'Tunisia', 'Tunis')\n",
      "('contains', 'Brickfields', 'Kuala_Lumpur_Sentral_railway_station')\n",
      "('film_performance', 'Colin_Hanks', 'The_Great_Buck_Howard')\n",
      "('founders', 'Bomis', 'Jimmy_Wales')\n",
      "('genre', 'SPARQL', 'Semantic_Web')\n",
      "('has_sibling', 'Ari_Emanuel', 'Rahm_Emanuel')\n",
      "('has_spouse', 'Percy_Bysshe_Shelley', 'Mary_Shelley')\n",
      "('is_a', 'Bhanu_Athaiya', 'Costume_designer')\n",
      "('nationality', 'Ruben_Rausing', 'Sweden')\n",
      "('parents', 'Prince_Arthur_of_Connaught', 'Prince_Arthur,_Duke_of_Connaught_and_Strathearn')\n",
      "('place_of_birth', 'William_Penny_Brookes', 'Much_Wenlock')\n",
      "('place_of_death', 'Jean_Drapeau', 'Montreal')\n",
      "('profession', 'Rufus_Wainwright', 'Actor')\n",
      "('worked_at', 'Ray_Jackendoff', 'Tufts_University')\n"
     ]
    }
   ],
   "source": [
    "for rel in all_relations:\n",
    "    print(tuple(kb.get_triples_for_relation(rel)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "# need to download the following files yourself since they are too big to be uploaded\n",
    "# download from: https://nlp.stanford.edu/software/lex-parser.shtml#Download\n",
    "# please refer this for more details: https://stackoverflow.com/questions/7443330/how-do-i-do-dependency-parsing-in-nltk \n",
    "\n",
    "path_to_jar = '/Users/hujiayu/Downloads/stanford-parser-full-2014-08-27/stanford-parser.jar'\n",
    "path_to_models_jar = '/Users/hujiayu/Downloads/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'\n",
    "\n",
    "dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('play', 'VB'), 'aux', ('did', 'VBD')),\n",
       " (('play', 'VB'), 'nsubj', ('shaq', 'NNP')),\n",
       " (('play', 'VB'), 'prep', ('for', 'IN')),\n",
       " (('for', 'IN'), 'pobj', ('Who', 'WP'))]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# result = dependency_parser.raw_parse('I shot an elephant in my sleep')\n",
    "# Example: [Who] did [shaq] first play for ?\n",
    "\n",
    "result = dependency_parser.raw_parse('Who did shaq play for')\n",
    "dep = result.__next__()\n",
    "list(dep.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('makes', 'VBZ'), 'nsubj', ('Bell', 'NNP')),\n",
       " (('Bell', 'NNP'), 'vmod', ('based', 'VBN')),\n",
       " (('based', 'VBN'), 'prep', ('in', 'IN')),\n",
       " (('in', 'IN'), 'pobj', ('Angeles', 'NNP')),\n",
       " (('Angeles', 'NNP'), 'nn', ('Los', 'NNP')),\n",
       " (('makes', 'VBZ'), 'cc', ('and', 'CC')),\n",
       " (('makes', 'VBZ'), 'conj', ('distributes', 'VBZ')),\n",
       " (('makes', 'VBZ'), 'dobj', ('products', 'NNS')),\n",
       " (('products', 'NNS'), 'amod', ('electronic', 'JJ')),\n",
       " (('electronic', 'JJ'), 'conj', ('computer', 'NN')),\n",
       " (('electronic', 'JJ'), 'cc', ('and', 'CC')),\n",
       " (('electronic', 'JJ'), 'conj', ('building', 'NN'))]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list(dep.triples())\n",
    "result = dependency_parser.raw_parse('Bell, based in Los Angeles, \\\n",
    "                                    makes and distributes electronic, \\\n",
    "                                    computer and building products')\n",
    "dep = result.__next__()\n",
    "list(dep.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking for word:  shaq\n",
      "found word here:  (('play', 'VB'), 'nsubj', ('shaq', 'NNP'))\n",
      "looking for word:  play\n",
      "found word here:  (('play', 'VB'), 'aux', ('did', 'VBD'))\n",
      "looking for word:  did\n",
      "couldn't find word:  did\n",
      "getting rid of word:  did\n",
      "looking for word:  play\n",
      "found word here:  (('play', 'VB'), 'prep', ('for', 'IN'))\n",
      "looking for word:  for\n",
      "found word here:  (('for', 'IN'), 'pobj', ('Who', 'WP'))\n",
      "looking for word:  Who\n",
      "['shaq', 'play', 'for', 'Who']\n",
      "['nsubj', 'aux', 'prep', 'pobj']\n",
      "['right', 'left', 'left', 'left']\n"
     ]
    }
   ],
   "source": [
    "# syntactic features\n",
    "\n",
    "# pobj - prep - play - nsubj \n",
    "\n",
    "\n",
    "# to use: \n",
    "# word_q, label_path, edge_path = get_shortest_path(list(dep.triples()), mention1, mention2)\n",
    "\n",
    "\"\"\"\n",
    "parser example: 'Who did shaq play for'\n",
    "\n",
    "[(('play', 'VB'), 'aux', ('did', 'VBD')),\n",
    " (('play', 'VB'), 'nsubj', ('shaq', 'NNP')),\n",
    " (('play', 'VB'), 'prep', ('for', 'IN')),\n",
    " (('for', 'IN'), 'pobj', ('Who', 'WP'))]\n",
    " \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_shortest_path(tree, mention, question_word):\n",
    "    \"\"\"\n",
    "    returns word_path, label_path and edge_path from two mention words\n",
    "    the entity mention and the question word are excluded from the dependency path \n",
    "   \n",
    "    \"\"\"\n",
    "    label_path = []\n",
    "    edge_path = []\n",
    "    visited = [0]*len(tree)\n",
    "    \n",
    "    def find_word(curr_w):\n",
    "        print(\"looking for word: \", curr_w)\n",
    "        if curr_w == question_word: return True\n",
    "        for idx, s in enumerate(tree):\n",
    "            if visited[idx] == 0 and s[0][0] == curr_w:\n",
    "                print('found word here: ', s)\n",
    "                if s[0][0] not in word_q:\n",
    "                    word_q.append(s[0][0])\n",
    "                label_path.append(s[1])\n",
    "                edge_path.append('left')\n",
    "                word_q.append(s[2][0])\n",
    "                visited[idx] = 1\n",
    "                return True\n",
    "            if visited[idx] == 0 and s[2][0] == curr_w:\n",
    "                print('found word here: ', s)\n",
    "                if s[2][0] not in word_q:\n",
    "                    word_q.append(s[2][0])\n",
    "                label_path.append(s[1])\n",
    "                edge_path.append('right')\n",
    "                word_q.append(s[0][0])\n",
    "                visited[idx] = 1\n",
    "                return True\n",
    "        print(\"couldn't find word: \", curr_w)\n",
    "        return False\n",
    "    \n",
    "    word_q = [mention]\n",
    "    \n",
    "    T = True\n",
    "    while T:        \n",
    "        k = word_q[-1]\n",
    "        if k == question_word: \n",
    "            T = False\n",
    "        if find_word(k) == False: \n",
    "            print('getting rid of word: ', k)\n",
    "            word_q.pop()\n",
    "            \n",
    "    return word_q, label_path, edge_path\n",
    "                                             \n",
    "mention1 = 'shaq'\n",
    "mention2 = 'Who'\n",
    "        \n",
    "word_q, label_path, edge_path = get_shortest_path(list(dep.triples()), mention1, mention2)\n",
    "\n",
    "print(word_q)\n",
    "print(label_path)\n",
    "print(edge_path)\n",
    "\n",
    "def extract_feature(word_q, label_path, edge_path):\n",
    "    \"\"\"\n",
    "    returns list of features to be represented in word representation\n",
    "    \n",
    "    treats the path as a concatenation of vectors or words, dependency edge directions and dependency labels\n",
    "    and feed it to the convolution layer\n",
    "    \n",
    "    extracting features from word_path, label_path, edge_path\n",
    "    \n",
    "    ['shaq', 'play', 'for', 'Who']\n",
    "    ['nsubj', 'aux', 'prep', 'pobj']\n",
    "    ['right', 'left', 'left', 'left']\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def get_word_repre(sentence):\n",
    "    # vectors or words\n",
    "    pass\n",
    "\n",
    "\n",
    "def concat():\n",
    "    \n",
    " \n",
    "    pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentential features\n",
    "# takes the words in the sentence as input excluding the question word and the entity mention\n",
    "\n",
    "# did first play for\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
