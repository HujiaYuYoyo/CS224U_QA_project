{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntactic Features\n",
    "\n",
    "# We use the shortest path between an entity mention and the question word in the dependency tree\n",
    "# as input to the first channel.\n",
    "\n",
    "\"\"\"\n",
    "parser example: 'Who did shaq play for'\n",
    "\n",
    "[(('play', 'VB'), 'aux', ('did', 'VBD')),\n",
    " (('play', 'VB'), 'nsubj', ('shaq', 'NNP')),\n",
    " (('play', 'VB'), 'prep', ('for', 'IN')),\n",
    " (('for', 'IN'), 'pobj', ('Who', 'WP'))]\n",
    " \n",
    "\"\"\"\n",
    "import nltk\n",
    "from nltk.parse.stanford import StanfordDependencyParser\n",
    "\n",
    "def set_parser(path_to_jar, path_to_models_jar):\n",
    "    \"\"\"\n",
    "    imports parser and sets parser just once for future use\n",
    "    \"\"\"\n",
    "    \n",
    "    dependency_parser = StanfordDependencyParser(path_to_jar=path_to_jar, path_to_models_jar=path_to_models_jar)\n",
    "\n",
    "    return dependency_parser\n",
    "    \n",
    "    \n",
    "def get_tree(dependency_parser, sentence):\n",
    "    \"\"\"\n",
    "    uses the dependency parser imported in set_parser to parse the given sentence into a tree\n",
    "    returns a list of triples like the following:\n",
    "    \n",
    "    parser example: 'Who did shaq play for'\n",
    "\n",
    "    [(('play', 'VB'), 'aux', ('did', 'VBD')),\n",
    "     (('play', 'VB'), 'nsubj', ('shaq', 'NNP')),\n",
    "     (('play', 'VB'), 'prep', ('for', 'IN')),\n",
    "     (('for', 'IN'), 'pobj', ('Who', 'WP'))]\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"parsing sentence: \", sentence)\n",
    "    result = dependency_parser.raw_parse(sentence)\n",
    "    dep = result.__next__()\n",
    "    \n",
    "    return list(dep.triples())\n",
    "\n",
    "\n",
    "def get_shortest_path(tree, mention, question_word, verbose = False):\n",
    "    \"\"\"\n",
    "    this function gets the shortest path from a parsed tree from running\n",
    "    \n",
    "    result = dependency_parser.raw_parse('Who did shaq play for')\n",
    "    dep = result.__next__()\n",
    "    tree = list(dep.triples())\n",
    "    \n",
    "    mention = entity_word\n",
    "    question_word = Who/Where/Why/How\n",
    "    \n",
    "    example of using this function:\n",
    "    \n",
    "    mention1 = 'shaq'\n",
    "    mention2 = 'Who'\n",
    "    result = dependency_parser.raw_parse('Who did shaq play for')\n",
    "    dep = result.__next__()\n",
    "\n",
    "    word_q, label_path, edge_path = get_shortest_path(list(dep.triples()), mention1, mention2)\n",
    "\n",
    "    returns word_path, label_path and edge_path from two mentioned words\n",
    "   \n",
    "    \"\"\"\n",
    "    label_path = []\n",
    "    edge_path = []\n",
    "    visited = [0]*len(tree)\n",
    "    \n",
    "    def find_word(curr_w):\n",
    "        if verbose: print(\"looking for word: \", curr_w)\n",
    "        if curr_w == question_word: return True\n",
    "        for idx, s in enumerate(tree):\n",
    "            if visited[idx] == 0 and s[0][0] == curr_w:\n",
    "                if verbose: print('found word here: ', s)\n",
    "                if s[0][0] not in word_q:\n",
    "                    word_q.append(s[0][0])\n",
    "                label_path.append(s[1])\n",
    "                edge_path.append('left')\n",
    "                word_q.append(s[2][0])\n",
    "                visited[idx] = 1\n",
    "                return True\n",
    "            if visited[idx] == 0 and s[2][0] == curr_w:\n",
    "                if verbose: print('found word here: ', s)\n",
    "                if s[2][0] not in word_q:\n",
    "                    word_q.append(s[2][0])\n",
    "                label_path.append(s[1])\n",
    "                edge_path.append('right')\n",
    "                word_q.append(s[0][0])\n",
    "                visited[idx] = 1\n",
    "                return True\n",
    "        if verbose: print(\"couldn't find word: \", curr_w)\n",
    "        return False\n",
    "    \n",
    "    word_q = [mention]\n",
    "    \n",
    "    T = True\n",
    "    while T:        \n",
    "        k = word_q[-1]\n",
    "        if k == question_word: \n",
    "            T = False\n",
    "        if find_word(k) == False: \n",
    "            if verbose: print('getting rid of word: ', k)\n",
    "            word_q.pop()\n",
    "            \n",
    "    return word_q, label_path, edge_path\n",
    "          \n",
    "    \n",
    "    \n",
    "def extract_feature(word_q, label_path, edge_path, mention1, mention2):\n",
    "    \"\"\"\n",
    "    this function takes in parsed word_path, label_path, edge_path from the above function and \n",
    "    returns list of features to be represented using GloVe and to be fed into the model as our features\n",
    "    \n",
    "    treats the path as a concatenation of vectors or words, dependency edge directions and dependency labels\n",
    "    and feed it to the convolution layer\n",
    "    \n",
    "    extracting features from word_path, label_path, edge_path\n",
    "    example:\n",
    "    \n",
    "    inputs: (from calling the above function)\n",
    "    \n",
    "    word_q: ['shaq', 'play', 'for', 'Who']\n",
    "    label_path: ['nsubj', 'aux', 'prep', 'pobj']\n",
    "    edge_path: ['right', 'left', 'left', 'left']\n",
    "    \n",
    "    output: ['left', 'nsubj', 'middle', 'play', 'middle', 'for', 'middle', 'pobj', 'right']\n",
    "\n",
    "    \"\"\"\n",
    "    entities = set([mention1, mention2])\n",
    "    path = ['left']\n",
    "    for idx, val in enumerate(word_q):\n",
    "        if val in entities:\n",
    "            path.append(label_path[idx])\n",
    "            if idx == 0:\n",
    "                path.append('middle')\n",
    "        else:\n",
    "            path.append(val)\n",
    "            path.append('middle')\n",
    "    path.append('right')\n",
    "    assert len(path) == len(word_q) * 2+1\n",
    "    return path\n",
    "    \n",
    "    \n",
    "    \n",
    "def load_GloVe(gloveFile):\n",
    "    \"\"\"\n",
    "    loads pre_trained glove vectors\n",
    "    \n",
    "    example file name: \n",
    "    /Users/hujiayu/Documents/GitHub/cs224u/vsmdata/glove.6B/glove.6B.50d.txt\n",
    "    \n",
    "    can use one of the following dimensions:\n",
    "    \n",
    "    glove.6B.50d.txt\n",
    "    glove.6B.100d.txt\n",
    "    glove.6B.200d.txt\n",
    "    glove.6B.300d.txt\n",
    "    \n",
    "    returns a dictionary with \n",
    "    key = word, and value = vector representation\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Loading GloVe vectors..\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model\n",
    "    \n",
    "    \n",
    "    \n",
    "def get_word_repre(features, glove_model, dim):\n",
    "    \"\"\"\n",
    "    This function takes in a list of words and represent them using \n",
    "    word vector representations from GloVe\n",
    "    append to feature vector a random normal initialized vector is not found in glove\n",
    "    \n",
    "    example input: \n",
    "    \n",
    "    path = ['left', 'nsubj', 'middle', 'play', 'middle', 'for', 'middle', 'pobj', 'right']\n",
    "    \n",
    "    returns np array matrix of dimension len(features) x dimension of GloVe used\n",
    "    \n",
    "    \"\"\"\n",
    "    res = []\n",
    "    \n",
    "    for w in features:\n",
    "        try:\n",
    "            res.append(glove_model[w])\n",
    "        except KeyError:\n",
    "            print('word %s is not in GloVe' % w)\n",
    "            if w in mapping:\n",
    "                print('adding word representation for %s instead '% convert_words(mapping, w))\n",
    "                res.append(glove_model[convert_words(mapping, w)])\n",
    "            else:\n",
    "                # create random normal vector\n",
    "                res.append(np.random.normal((1,dim)))\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "def convert_words(mapping, word):\n",
    "    \"\"\"\n",
    "    since many of the syntax words cannot be found in glove\n",
    "    convert to more general form such as nsubj - subject, and pobj - object\n",
    "    \"\"\"\n",
    "    return mapping[word]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing sentence:  Who did shaq first play for?\n",
      "Loading GloVe vectors..\n",
      "Done. 400000  words loaded!\n",
      "word nsubj is not in GloVe\n",
      "adding word representation for subject instead \n",
      "word pobj is not in GloVe\n",
      "adding word representation for object instead \n",
      "syntactic feature vector is of shape:  (9, 50)\n"
     ]
    }
   ],
   "source": [
    "# This cell uses the functions defined above and calls for syntactic features\n",
    "\n",
    "path_to_jar = '/Users/hujiayu/Downloads/stanford-parser-full-2014-08-27/stanford-parser.jar'\n",
    "path_to_models_jar = '/Users/hujiayu/Downloads/stanford-parser-full-2014-08-27/stanford-parser-3.4.1-models.jar'\n",
    "\n",
    "mapping = {'nsubj':'subject', 'pobj':'object', 'prep':'preposition', 'aux':'auxiliary'}\n",
    "\n",
    "gloveFile = '/Users/hujiayu/Documents/GitHub/cs224u/vsmdata/glove.6B/glove.6B.50d.txt'\n",
    "dim = 50\n",
    "\n",
    "sentence = 'Who did shaq first play for?'\n",
    "mention1 = 'shaq'\n",
    "mention2 = 'Who'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    dependency_parser = set_parser(path_to_jar, path_to_models_jar)\n",
    "    \n",
    "    tree = get_tree(dependency_parser, sentence)\n",
    "    \n",
    "    word_q, label_path, edge_path = get_shortest_path(tree, mention1, mention2, verbose = False)\n",
    "\n",
    "    \n",
    "    # print(word_q)\n",
    "    # print(label_path)\n",
    "    # print(edge_path)\n",
    "\n",
    "    features = extract_feature(word_q, label_path, edge_path, mention1, mention2)\n",
    "#     print(features)\n",
    "    glove_model = load_GloVe(gloveFile)\n",
    "    syntactic_feature = get_word_repre(features, glove_model, dim)\n",
    "    print('syntactic feature vector is of shape: ', feature_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentential feature is of shape:  (4, 50)\n"
     ]
    }
   ],
   "source": [
    "# Sentential Features\n",
    "# According to paper, this channel takes the words in the sentence as input excluding the question \n",
    "# word and the entity mention. For example above: who did shaq first play for ? \n",
    "# the vectors for did, first, play and for are fed into this channel.\n",
    "\n",
    "def sentential_feature(sentence, glove_model, mention1, mention2):\n",
    "    \"\"\"\n",
    "    this function takes in the raw sentence, pre_trained glove model, and two mention words\n",
    "    and outputs the glove word representation of all words in the sentence except the\n",
    "    mention words\n",
    "    assume input is a full sentence: 'Who did shaq first play for'\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    entities = set([mention1, mention2])\n",
    "    if sentence[-1] == '?':sentence = sentence[:-1]\n",
    "    sentence = sentence.split()\n",
    "    for w in sentence:\n",
    "        if w not in entities:\n",
    "            res.append(glove_model[w])\n",
    "    res = np.array(res)\n",
    "    print('sentential feature is of shape: ', res.shape)\n",
    "    return res\n",
    "\n",
    "sentential_feature = sentential_feature(sentence, glove_model, mention1, mention2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('play', 'VB'), 'aux', ('did', 'VBD')),\n",
       " (('play', 'VB'), 'nsubj', ('shaq', 'NNP')),\n",
       " (('play', 'VB'), 'prep', ('for', 'IN')),\n",
       " (('for', 'IN'), 'pobj', ('Who', 'WP'))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this cell shows some code snippets examples for using the packages\n",
    "\n",
    "\n",
    "# result = dependency_parser.raw_parse('I shot an elephant in my sleep')\n",
    "# Example: [Who] did [shaq] first play for ?\n",
    "\n",
    "result = dependency_parser.raw_parse('Who did shaq first play for')\n",
    "dep = result.__next__()\n",
    "list(dep.triples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 50)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MCCNNs for relation classification \n",
    "# now we combine both features and feed into a convolutional network\n",
    "\n",
    "# According to the paper, Convolution layer tackles an input of varying length returning a fixed length vector (we\n",
    "# use max pooling) for each channel. These fixed length vectors are concatenated and then fed into a\n",
    "# softmax classifier, the output dimension of which is equal to the number of predefined relation types.\n",
    "# The value of each dimension indicates the confidence score of the corresponding relation\n",
    "\n",
    "combined_feature = np.concatenate((sentential_feature, syntactic_feature), axis = 0)\n",
    "combined_feature.shape\n",
    "# but we combine these two features after convolutional layer not before ! \n",
    "# will work on this over the weekend - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
